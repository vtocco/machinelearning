# -*- coding: utf-8 -*-
"""Stock Price Predictor - Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JdUiden_qthn1oCDzmCWtrAnkb7BouZ2
"""

import numpy as np
from datetime import datetime
from datetime import timedelta
import pandas as pd
import sklearn
import matplotlib.pyplot as plt
from math import ceil
import seaborn as sns

from sklearn.svm import SVR

# 3 different regression models we are going to use for stock prediction
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LassoLars

from sklearn import preprocessing
from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error

from pandas_datareader import data as pdr
import yfinance as yf
yf.pdr_override()
df_full = pdr.get_data_yahoo("AXP", start="2018-01-01").reset_index()
#df_full.to_csv('AXP.csv',index=False)

#df_full = pd.read_csv('AXP.csv')
#df_full.head()
#df_full.tail()
#df_full.describe()
# make sure there is no missing info (counts should all be the same)
df_full.info()

# create the lists and x,y dataset
dates = []
prices = []


corr = df_full.corr()
sns.heatmap(corr, annot=True)

# Clean up the data and just use adjusted close and volume
#df = df_full.loc[:,['Adj Close', 'Volume']]

#calculate the high minus low percentage
#df['HL_PCT'] = (df_full['High'] - df_full['Low']) / df_full['Close'] * 100

# calculate the percentage change
#df['PCT_change'] = (df_full['Close'] - df_full['Open']) / df_full['Open'] * 100
stock_data = df_full

# clean missing data
# df.fillna(value=-99999, inplace=True)
stock_data.dropna(inplace=True)
stock_data.head()

df_test = stock_data[-20:]

print(df_test.shape)
df_test.head()

df_train = stock_data[:-20]
df_train.head()

# Plotting Train and Test data
fig = plt.figure(figsize=[10,8])
ax = plt.subplot(111)
ax.plot(df_train['Adj Close'], label='Train')
ax.plot(df_test['Adj Close'], label='Valid')
ax.legend()
plt.show()

# modify the data
window = 7                       # We will use the last 6 day data to make prediction

train_data = df_train['Adj Close']
test_data = df_test['Adj Close']

#set it back to zero based
test_data = test_data.reset_index(drop=True)

# put the data into a dataframe that we will use for the model
# Training data
print("Training Data")
index = len(train_data) - window

data = pd.DataFrame(np.zeros((index, window)))

for row in range(index):
  for col in range(window):
    data.iloc[row,col] = train_data[col+row]
print(data.tail())

y_train = data.iloc[:,-1]
print(y_train.shape)

X_train = data.iloc[:,:-1]
print(X_train.shape)



#test data
print("Test Data")
index = len(test_data) - window

# this shows a 7 x 13 array or zeros
data = pd.DataFrame(np.zeros((index, window)))
print("test data: ", index, window, test_data[0])

for row in range(index):
  for col in range(window):
    data.iloc[row,col] = test_data[col+row]
print(data.tail())

y_test = data.iloc[:,-1]
print(y_test.shape)

X_test = data.iloc[:,:-1]
print(X_test.shape)

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
mse = mean_squared_error(y_test, y_pred_lr)
print("Root Mean Squared Error: ", np.sqrt(mse))

fig = plt.figure(figsize=[10,8])
ax = plt.subplot(111)
ax.plot(y_test.index, y_pred_lr, label='Predicted')
ax.plot(y_test, label='Test')
ax.legend()
plt.show()

print("Predicted: ", y_pred_lr)
print("Actual: ", y_test)

#quadratic regression 2
poly2 = make_pipeline(PolynomialFeatures(2), Ridge())
poly2.fit(X_train, y_train)

poly2_pred = poly2.predict(X_test)

mse = mean_squared_error(y_test, poly2_pred)
print("Root Mean Squared Error: ", np.sqrt(mse))

fig = plt.figure(figsize=[10,8])
ax = plt.subplot(111)
ax.plot(y_test.index, poly2_pred, label='Predicted')
ax.plot(y_test, label='Test')
ax.legend()
plt.show()

#quadratic regression 3
poly3 = make_pipeline(PolynomialFeatures(3), Ridge())
poly3.fit(X_train, y_train)

poly3_pred = poly3.predict(X_test)

mse = mean_squared_error(y_test, poly3_pred)
print("Root Mean Squared Error: ", np.sqrt(mse))

fig = plt.figure(figsize=[10,8])
ax = plt.subplot(111)
ax.plot(y_test.index, poly3_pred, label='Predicted')
ax.plot(y_test, label='Test')
ax.legend()
plt.show()

#KNN Regression
knn=KNeighborsRegressor(n_neighbors=2)
knn.fit(X_train, y_train)

knn_pred = knn.predict(X_test)

mse = mean_squared_error(y_test, knn_pred)
print("Root Mean Squared Error: ", np.sqrt(mse))

fig = plt.figure(figsize=[10,8])
ax = plt.subplot(111)
ax.plot(y_test.index, knn_pred, label='Predicted')
ax.plot(y_test, label='Test')
ax.legend()
plt.show()

# Lasso lars
lasso = LassoLars(alpha=.1)
lasso.fit(X_train, y_train)

lasso_pred = lasso.predict(X_test)

mse = mean_squared_error(y_test, lasso_pred)
print("Root Mean Squared Error: ", np.sqrt(mse))

fig = plt.figure(figsize=[10,8])
ax = plt.subplot(111)
ax.plot(y_test.index, lasso_pred, label='Predicted')
ax.plot(y_test, label='Test')
ax.legend()
plt.show()

# Evaluation
confidence_lr = lr.score(X_test, y_test)
confidence_poly2 = poly2.score(X_test, y_test)
confidence_poly3 = poly3.score(X_test, y_test)
confidence_knn = knn.score(X_test, y_test)
confidence_lasso = lasso.score(X_test, y_test)

print("Results: ", confidence_lr,confidence_poly2, confidence_poly3, confidence_knn, confidence_lasso )

# all on one graph

fig = plt.figure(figsize=[10,8])
ax = plt.subplot(111)
ax.plot(y_test.index, lasso_pred, label='Lasso', color='red')
ax.plot(y_test.index, knn_pred, label='KNN', color='blue')
ax.plot(y_test.index, poly2_pred, label='Poly2', color='green')
ax.plot(y_test.index, poly3_pred, label='Poly3', color='orange')
ax.plot(y_test.index, y_pred_lr, label='LR', color='cyan')
ax.plot(y_test, label='Test', color='magenta')
ax.legend()
plt.show()